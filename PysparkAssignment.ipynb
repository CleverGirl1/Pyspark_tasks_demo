{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\sijal\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\sijal\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\sijal\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\sijal\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sijal\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sijal\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sijal\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sijal\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sijal\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\sijal\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sijal\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sijal\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sijal\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sijal\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sijal\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"BDM-3603-Assignment1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "# Read the CSV file with the correct delimiter\n",
    "data = spark.read.csv(\"online_retail.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Show the schema to verify\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the columns are still combined, split them\n",
    "if len(Data.columns) == 1:\n",
    "    column_names = [\"InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\", \"CustomerID\", \"Country\"]\n",
    "    Data = Data.select(split(col(Data.columns[0]), \",\").alias(\"temp\"))\n",
    "    for i, name in enumerate(column_names):\n",
    "        Data = Data.withColumn(name, col(\"temp\")[i])\n",
    "    Data = Data.drop(\"temp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 541909\n"
     ]
    }
   ],
   "source": [
    "total_rows = data.count()\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Data Cleaning and Transformation\n",
    "# Drop duplicates\n",
    "data = data.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country']\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|        0|        0|          0|       0|          0|        0|         0|      0|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check which colums have null values\n",
    "\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "null_counts = data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|        0|        0|          0|       0|          0|        0|         0|      0|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check which columns have missing values\n",
    "\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "missing_counts = data.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in data.columns])\n",
    "missing_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill missing Description with \"No Description\" and CustomerID with -1\n",
    "data = data.fillna({\"Description\": \"No Description\", \"CustomerID\": -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# Here, we fill missing numeric values with 0 and string values with \"Unknown\" (modify as necessary) but this is not necessary for our case\n",
    "from pyspark.sql.types import NumericType, StringType\n",
    "\n",
    "# Identify numeric and string columns\n",
    "numeric_columns = [f.name for f in data.schema.fields if isinstance(f.dataType, NumericType)]\n",
    "string_columns = [f.name for f in data.schema.fields if isinstance(f.dataType, StringType)]\n",
    "\n",
    "# Fill missing values\n",
    "df_filled = data.fillna(0, subset=numeric_columns).fillna(\"Unknown\", subset=string_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import IntegerType, DoubleType, DateType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Convert data types and handle missing values\n",
    "# Convert Quantity to Integer, UnitPrice to Double, and InvoiceDate to Date\n",
    "data = data.withColumn(\"Quantity\", data[\"Quantity\"].cast(IntegerType())) \\\n",
    "           .withColumn(\"UnitPrice\", data[\"UnitPrice\"].cast(DoubleType())) \\\n",
    "           .withColumn(\"InvoiceDate\", F.to_date(data[\"InvoiceDate\"], \"MM/dd/yyyy\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Filtering Out Invalid Rows\n",
    "# Example: Filter out rows where Quantity or UnitPrice are non-positive\n",
    "data = data.filter((data[\"Quantity\"] > 0) & (data[\"UnitPrice\"] > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If normalization is needed, we can scale columns using mean normalization\n",
    "quantity_mean = data.select(F.mean(\"Quantity\")).first()[0]\n",
    "quantity_stddev = data.select(F.stddev(\"Quantity\")).first()[0]\n",
    "\n",
    "data = data.withColumn(\"Quantity_Normalized\", \n",
    "                       (data[\"Quantity\"] - quantity_mean) / quantity_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = false)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: date (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = false)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Quantity_Normalized: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3. Data Analysis Using Spark SQL\n",
    "\n",
    "# Register the cleaned DataFrame as a temporary SQL view\n",
    "data.createOrReplaceTempView(\"invoice_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+------------------+------------------+-----------------+-----------------+\n",
      "|      avg_quantity|median_quantity|   stddev_quantity|    avg_unit_price|median_unit_price|stddev_unit_price|\n",
      "+------------------+---------------+------------------+------------------+-----------------+-----------------+\n",
      "|10.542037034242338|            3.0|155.52412351063617|3.9076252471218242|             2.08|35.91568110425546|\n",
      "+------------------+---------------+------------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Aggregation: Summary Statistics\n",
    "summary_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        AVG(Quantity) AS avg_quantity,\n",
    "        PERCENTILE(Quantity, 0.5) AS median_quantity,\n",
    "        STDDEV(Quantity) AS stddev_quantity,\n",
    "        AVG(UnitPrice) AS avg_unit_price,\n",
    "        PERCENTILE(UnitPrice, 0.5) AS median_unit_price,\n",
    "        STDDEV(UnitPrice) AS stddev_unit_price\n",
    "    FROM invoice_data\n",
    "\"\"\")\n",
    "summary_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+------------------+\n",
      "|        Country|       total_sales|    avg_unit_price|\n",
      "+---------------+------------------+------------------+\n",
      "| United Kingdom| 9025222.084000133| 3.849679429753172|\n",
      "|    Netherlands|         285446.34|2.6484654514624855|\n",
      "|           EIRE| 283453.9600000004|4.8783206590620924|\n",
      "|        Germany|228867.13999999987| 3.709307522123887|\n",
      "|         France|209715.11000000004| 4.400236707505655|\n",
      "|      Australia|138521.30999999997|3.0562605752961063|\n",
      "|          Spain|          61577.11| 3.826223832528186|\n",
      "|    Switzerland| 57089.89999999997|3.3745473041709064|\n",
      "|        Belgium| 41196.34000000002|3.6301575578532757|\n",
      "|         Sweden|          38378.33|3.7600665188470073|\n",
      "|          Japan|          37416.37|2.0473831775700937|\n",
      "|         Norway|          36165.44| 5.287086834733894|\n",
      "|       Portugal|33747.100000000006| 5.843251165889407|\n",
      "|        Finland|22546.079999999998| 5.296992700729926|\n",
      "|      Singapore|          21279.29| 58.33328828828829|\n",
      "|Channel Islands|20450.440000000002| 4.531617647058824|\n",
      "|        Denmark|18955.339999999997| 3.146184210526316|\n",
      "|          Italy|          17483.24| 4.717955145118734|\n",
      "|      Hong Kong|15691.799999999994| 23.47443661971831|\n",
      "|         Cyprus|13590.380000000001| 5.710390879478833|\n",
      "+---------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Grouping and Filtering: Group data by 'Country' and calculate total sales and average price\n",
    "country_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Country,\n",
    "        SUM(Quantity * UnitPrice) AS total_sales,\n",
    "        AVG(UnitPrice) AS avg_unit_price\n",
    "    FROM invoice_data\n",
    "    GROUP BY Country\n",
    "    HAVING total_sales > 10000\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\")\n",
    "country_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Valuable Insights from the above aggregation and grouping:\n",
    "\n",
    "1) Big Spread in Order Sizes: The average order quantity is 10.5, but the median is only 3, with a large standard deviation. This shows that while most orders are small, a few very large ones are pushing up the average.\n",
    "\n",
    "2) UK Leads in Sales: The UK has by far the highest total sales at over 9 million, much more than any other country. Its average unit price is close to the global average, suggesting the UK is the main market, with possible growth opportunities in places like Australia and Germany.\n",
    "\n",
    "3) High Price Differences by Country: Some countries, like Singapore and Hong Kong, have noticeably higher average prices, likely due to unique customer demands or higher-value products in those regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = false)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = false)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Quantity_Normalized: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = false)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = false)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Quantity_Normalized: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Convert 'InvoiceDate' to a timestamp\n",
    "data = data.withColumn(\"InvoiceDate\", to_timestamp(\"InvoiceDate\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Check the schema to confirm it's now in the correct format\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------+----------+--------------+--------------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|UnitPrice|CustomerID|       Country| Quantity_Normalized|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+--------------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|     2.55|     17850|United Kingdom|-0.02920471070156...|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|     3.39|     17850|United Kingdom|-0.02920471070156...|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|     2.75|     17850|United Kingdom|-0.01634496936463...|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|     3.39|     17850|United Kingdom|-0.02920471070156...|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|     3.39|     17850|United Kingdom|-0.02920471070156...|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove the 'InvoiceDate' column from the DataFrame. For some reason my dataframe would not function properly with the invoice date. Will look into it further.\n",
    "data = data.drop(\"InvoiceDate\")\n",
    "\n",
    "# Show the first 5 rows to verify\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a new column 'total_sales' by multiplying 'Quantity' and 'UnitPrice'\n",
    "New_data = data.withColumn(\"total_sales\", col(\"Quantity\") * col(\"UnitPrice\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------+----------+--------------+--------------------+------------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|UnitPrice|CustomerID|       Country| Quantity_Normalized|       total_sales|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+--------------------+------------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|     2.55|     17850|United Kingdom|-0.02920471070156...|15.299999999999999|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|     3.39|     17850|United Kingdom|-0.02920471070156...|             20.34|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|     2.75|     17850|United Kingdom|-0.01634496936463...|              22.0|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|     3.39|     17850|United Kingdom|-0.02920471070156...|             20.34|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|     3.39|     17850|United Kingdom|-0.02920471070156...|             20.34|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "New_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_data = New_data.na.drop(subset=[\"Country\", \"Description\", \"Quantity\", \"UnitPrice\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = New_data.na.drop(column=[\"Country\", \"Quantity\", \"UnitPrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------+----------+--------------+--------------------+------------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|UnitPrice|CustomerID|       Country| Quantity_Normalized|       total_sales|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+--------------------+------------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|     2.55|     17850|United Kingdom|-0.02920471070156...|15.299999999999999|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|     3.39|     17850|United Kingdom|-0.02920471070156...|             20.34|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|     2.75|     17850|United Kingdom|-0.01634496936463...|              22.0|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|     3.39|     17850|United Kingdom|-0.02920471070156...|             20.34|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|     3.39|     17850|United Kingdom|-0.02920471070156...|             20.34|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the data into training and test sets again\n",
    "train_data, test_data = New_data.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "\n",
    "#Encoding the 'Country' column\n",
    "# Use StringIndexer to convert 'Country' to a numeric format\n",
    "indexer_country = StringIndexer(inputCol=\"Country\", outputCol=\"CountryIndex\")\n",
    "encoder_country = OneHotEncoder(inputCol=\"CountryIndex\", outputCol=\"CountryVec\")\n",
    "\n",
    "\n",
    "# OneHotEncoder: Convert numerical indices into one-hot encoded vectors\n",
    "encoder_country = OneHotEncoder(inputCol=\"CountryIndex\", outputCol=\"CountryVec\")\n",
    "encoder_description = OneHotEncoder(inputCol=\"DescriptionIndex\", outputCol=\"DescriptionVec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  Assemble features into a single vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"CountryVec\", \"Quantity\", \"UnitPrice\"],  # Only these columns for features\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"total_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (70% training, 30% testing)\n",
    "train_data_split, test_data_split = train_data.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[indexer_country, encoder_country, assembler, regressor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model on the training data\n",
    "model = pipeline.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 436.76738139297873\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"total_sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------+----------+--------------+--------------------+------------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|UnitPrice|CustomerID|       Country| Quantity_Normalized|       total_sales|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+--------------------+------------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|     2.55|     17850|United Kingdom|-0.02920471070156...|15.299999999999999|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|     3.39|     17850|United Kingdom|-0.02920471070156...|             20.34|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|     2.75|     17850|United Kingdom|-0.01634496936463...|              22.0|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|     3.39|     17850|United Kingdom|-0.02920471070156...|             20.34|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|     3.39|     17850|United Kingdom|-0.02920471070156...|             20.34|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Step 1: Prepare the features\n",
    "# First, we need to index the 'Country' column\n",
    "indexer = StringIndexer(inputCol=\"Country\", outputCol=\"CountryIndex\")\n",
    "\n",
    "# Then, we'll use OneHotEncoder to create dummy variables\n",
    "encoder = OneHotEncoder(inputCols=[\"CountryIndex\"], outputCols=[\"CountryVec\"])\n",
    "\n",
    "# Assemble the features into a single vector column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"CountryVec\", \"UnitPrice\", \"Quantity\"],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create the model\n",
    "rf = RandomForestRegressor(labelCol=\"total_sales\", featuresCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Set up the parameter grid for tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [1, 2, 4]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Set up the cross-validator\n",
    "evaluator = RegressionEvaluator(labelCol=\"total_sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3)  # Use 3-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Fit the model\n",
    "cvModel = cv.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Make predictions on the test data\n",
    "predictions = cvModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 65.46127173534236\n",
      "R-squared (R2) on test data = 0.3707788975838975\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Evaluate the model\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "r2 = RegressionEvaluator(labelCol=\"total_sales\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(predictions)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "print(f\"R-squared (R2) on test data = {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: Country, Importance: 0.03368380625729595\n",
      "Feature: UnitPrice, Importance: 0.0002073017789438663\n",
      "Feature: Quantity, Importance: 0.009288465532555855\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Show feature importances\n",
    "bestModel = cvModel.bestModel\n",
    "featureImportances = bestModel.stages[-1].featureImportances\n",
    "featureNames = [\"Country\"] + [\"UnitPrice\", \"Quantity\"]\n",
    "for feature, importance in zip(featureNames, featureImportances):\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Show a sample of predictions\n",
    "predictions.select(\"total_sales\", \"prediction\", \"Country\", \"UnitPrice\", \"Quantity\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
